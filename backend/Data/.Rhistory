html_nodes("div.contents a") %>%
html_attr("href") %>%
str_c("https://nc.milesplit.com", .) %>% # Make full URLs
unique()
# Recursively scrape tables from these additional links
all_tables <- additional_links %>%
map(scrape_tables) %>%
flatten()
return(all_tables)
}
# Function to scrape all links on the main page and apply the table scraping process
scrape_all_results <- function(base_url) {
message("Scraping base URL: ", base_url)
base_page <- tryCatch({
read_html(base_url)
}, error = function(e) {
message("Error reading base page: ", base_url)
return(NULL)
})
if (is.null(base_page)) stop("Failed to read the base page. Exiting.")
# Extract all result page links
main_links <- base_page %>%
html_nodes("div.resultsList a, div.contents a") %>%
html_attr("href") %>%
str_c("https://nc.milesplit.com", .) %>%
unique()
message("Found ", length(main_links), " links to process.")
# Iterate through each link and scrape tables
all_results <- main_links %>%
map(function(link) {
message("Processing link: ", link)
scrape_tables(link)
}) %>%
flatten()
# Save each table to a CSV file
if (length(all_results) > 0) {
for (i in seq_along(all_results)) {
write.csv(all_results[[i]], file = paste0("table_", i, ".csv"), row.names = FALSE)
}
message("All tables saved as CSV files.")
} else {
message("No tables were found in any of the links.")
}
}
# Set base URL
base_url <- "https://nc.milesplit.com/results"
# Start the scraping process
scrape_all_results(base_url)
# Function to scrape tables from a single page
scrape_tables <- function(url) {
message("Scraping URL: ", url)
# Try to read the page
page <- tryCatch({
read_html(url)
}, error = function(e) {
message("Error reading page: ", url)
return(NULL)
})
# If the page fails to load, return NULL
if (is.null(page)) return(NULL)
# Search for tables globally on the page
tables <- page %>%
html_nodes("table") %>%
html_table(fill = TRUE)
if (length(tables) > 0) {
message("Found ", length(tables), " table(s) on the page.")
return(tables)
} else {
message("No tables found on this page.")
return(NULL)
}
}
# Function to scrape all result links from the main page
scrape_all_results <- function(base_url) {
message("Scraping base URL: ", base_url)
# Read the main page
base_page <- tryCatch({
read_html(base_url)
}, error = function(e) {
message("Error reading base page: ", base_url)
return(NULL)
})
if (is.null(base_page)) stop("Failed to read the base page. Exiting.")
# Extract all links to other pages
result_links <- base_page %>%
html_nodes("a") %>%                    # Extract all 'a' tags
html_attr("href") %>%                  # Get the href attributes
str_subset("/results/") %>%            # Keep only links related to results
str_c("https://nc.milesplit.com", .) %>% # Ensure full URLs
unique()
message("Found ", length(result_links), " result links to process.")
# Iterate through each link and scrape tables
all_tables <- result_links %>%
map(function(link) {
tables <- scrape_tables(link)
if (!is.null(tables)) {
return(tables)
} else {
return(NULL)
}
}) %>%
flatten() %>%                          # Combine all tables
compact()                              # Remove NULL entries
# Save each table to a CSV file
if (length(all_tables) > 0) {
for (i in seq_along(all_tables)) {
write.csv(all_tables[[i]], file = paste0("table_", i, ".csv"), row.names = FALSE)
}
message("All tables saved as CSV files.")
} else {
message("No tables were found on any of the links.")
}
}
# Set the base URL
base_url <- "https://nc.milesplit.com/results"
# Start the scraping process
scrape_all_results(base_url)
library(tibble)
# Function to extract tables from a single URL
extract_tables_from_url <- function(url) {
message("Processing URL: ", url)
tryCatch({
# Read the HTML content
page <- read_html(url)
# Extract tables, if any
tables <- page %>% html_nodes("table") %>% html_table(fill = TRUE)
# If tables are found, return them
if (length(tables) > 0) {
return(tables)
} else {
# If no tables, find sub-links within class="contents"
sub_links <- page %>%
html_nodes(".contents a") %>%
html_attr("href") %>%
str_c("https://nc.milesplit.com", .) %>%
unique()
# Recursively visit sub-links and extract tables
all_tables <- list()
for (sub_link in sub_links) {
sub_tables <- extract_tables_from_url(sub_link)
all_tables <- append(all_tables, sub_tables)
}
return(all_tables)
}
}, error = function(e) {
message("Error processing URL: ", url)
return(NULL)
})
}
# Main function to process the base page
process_base_page <- function(base_url) {
# Read the base page
base_page <- read_html(base_url)
# Extract all relevant links from class="contents"
links <- base_page %>%
html_nodes(".contents a") %>%
html_attr("href") %>%
str_c("https://nc.milesplit.com", .) %>%
unique()
# Initialize an empty list to store tables
all_tables <- list()
# Visit each link and extract tables
for (link in links) {
tables <- extract_tables_from_url(link)
if (!is.null(tables)) {
all_tables <- append(all_tables, tables)
}
}
return(all_tables)
}
# Run the scraping process
base_url <- "https://nc.milesplit.com/results"
all_extracted_tables <- process_base_page(base_url)
# Function to scrape tables with class 'eventTable' from a single URL
extract_event_tables <- function(url) {
message("Processing: ", url)
tryCatch({
# Read HTML content
page <- read_html(url)
# Find all tables with class 'eventTable'
tables <- page %>% html_nodes("table.eventTable") %>% html_table(fill = TRUE)
# Clean and return tables if found
if (length(tables) > 0) {
cleaned_tables <- map(tables, function(tbl) {
tbl <- as_tibble(tbl) %>%
rename_with(~ str_trim(.), everything()) %>% # Clean column names
mutate(across(everything(), ~str_squish(as.character(.)))) # Clean data
return(tbl)
})
return(cleaned_tables)
}
return(NULL)
}, error = function(e) {
message("Error processing: ", url)
return(NULL)
})
}
# Recursive function to scrape tables from a base URL and its sub-links
scrape_recursive <- function(base_url) {
# Read the base page
base_page <- read_html(base_url)
# Extract all sub-links with class 'contents'
sub_links <- base_page %>%
html_nodes(".contents a") %>%
html_attr("href") %>%
unique() %>%
na.omit()
# Prepend domain to relative links
sub_links <- str_c("https://nc.milesplit.com", sub_links)
# Initialize an empty list to store tables
all_tables <- list()
# Extract tables from the base URL
tables <- extract_event_tables(base_url)
if (!is.null(tables)) {
all_tables <- append(all_tables, tables)
}
# Loop through sub-links and extract tables recursively
for (link in sub_links) {
tables <- extract_event_tables(link)
if (!is.null(tables)) {
all_tables <- append(all_tables, tables)
}
}
return(all_tables)
}
# Base URL
base_url <- "https://nc.milesplit.com/results"
# Run the scraping process
all_extracted_tables <- scrape_recursive(base_url)
# Function to scrape tables with class 'eventTable' from a single URL
extract_event_tables <- function(url) {
message("Processing: ", url)
tryCatch({
# Read HTML content
page <- read_html(url)
# Find all tables with class 'eventTable'
tables <- page %>% html_nodes("eventTable") %>% html_table(fill = TRUE)
# Clean and return tables if found
if (length(tables) > 0) {
cleaned_tables <- map(tables, function(tbl) {
tbl <- as_tibble(tbl) %>%
rename_with(~ str_trim(.), everything()) %>% # Clean column names
mutate(across(everything(), ~str_squish(as.character(.)))) # Clean data
return(tbl)
})
return(cleaned_tables)
}
return(NULL)
}, error = function(e) {
message("Error processing: ", url)
return(NULL)
})
}
# Recursive function to scrape tables from a base URL and its sub-links
scrape_recursive <- function(base_url) {
# Read the base page
base_page <- read_html(base_url)
# Extract all sub-links with class 'contents'
sub_links <- base_page %>%
html_nodes(".contents a") %>%
html_attr("href") %>%
unique() %>%
na.omit()
# Prepend domain to relative links
sub_links <- str_c("https://nc.milesplit.com", sub_links)
# Initialize an empty list to store tables
all_tables <- list()
# Extract tables from the base URL
tables <- extract_event_tables(base_url)
if (!is.null(tables)) {
all_tables <- append(all_tables, tables)
}
# Loop through sub-links and extract tables recursively
for (link in sub_links) {
tables <- extract_event_tables(link)
if (!is.null(tables)) {
all_tables <- append(all_tables, tables)
}
}
return(all_tables)
}
# Base URL
base_url <- "https://nc.milesplit.com/results"
# Run the scraping process
all_extracted_tables <- scrape_recursive(base_url)
# Function to scrape tables with class 'eventTable' from a single URL
extract_event_tables <- function(url) {
message("Processing: ", url)
tryCatch({
# Read HTML content
page <- read_html(url)
# Find all tables with class 'eventTable'
tables <- page %>% html_nodes("table.eventTable") %>% html_table(fill = TRUE)
# Clean and return tables if found
if (length(tables) > 0) {
cleaned_tables <- map(tables, function(tbl) {
tbl <- as_tibble(tbl) %>%
rename_with(~ str_trim(.), everything()) %>% # Clean column names
mutate(across(everything(), ~str_squish(as.character(.)))) # Clean data
return(tbl)
})
return(cleaned_tables)
}
return(NULL)
}, error = function(e) {
message("Error processing: ", url)
return(NULL)
})
}
# Function to extract event links from the base page
extract_event_links <- function(base_url) {
message("Extracting event links from: ", base_url)
tryCatch({
# Read the base page
base_page <- read_html(base_url)
# Find links within <td class="name"> that have <a> tags
event_links <- base_page %>%
html_nodes("td.name a") %>%
html_attr("href") %>%
na.omit() %>%
unique()
# Prepend base domain to relative URLs
full_links <- str_c("https://nc.milesplit.com", event_links)
return(full_links)
}, error = function(e) {
message("Error extracting links: ", base_url)
return(NULL)
})
}
# Main function to process the base page and all event links
process_main_page <- function(base_url) {
# Extract all event links from the main page
event_links <- extract_event_links(base_url)
# Initialize an empty list to store tables
all_tables <- list()
# Visit each event link and extract tables
for (link in event_links) {
tables <- extract_event_tables(link)
if (!is.null(tables)) {
all_tables <- append(all_tables, tables)
}
}
return(all_tables)
}
# Base URL for the results page
base_url <- "https://nc.milesplit.com/results"
# Run the scraping process
all_extracted_tables <- process_main_page(base_url)
# Function to scrape tables with class 'eventTable' from a single URL
extract_event_tables <- function(url) {
message("Processing: ", url)
tryCatch({
# Read HTML content
page <- read_html(url)
# Find all tables with class 'eventTable'
tables <- page %>% html_nodes("table.eventTable")
# Clean and return tables if found
if (length(tables) > 0) {
cleaned_tables <- map(tables, function(tbl) {
tbl <- as_tibble(tbl) %>%
rename_with(~ str_trim(.), everything()) %>% # Clean column names
mutate(across(everything(), ~str_squish(as.character(.)))) # Clean data
return(tbl)
})
return(cleaned_tables)
}
return(NULL)
}, error = function(e) {
message("Error processing: ", url)
return(NULL)
})
}
# Function to extract event links from the base page
extract_event_links <- function(base_url) {
message("Extracting event links from: ", base_url)
tryCatch({
# Read the base page
base_page <- read_html(base_url)
# Find links within <td class="name"> that have <a> tags
event_links <- base_page %>%
html_nodes("td.name a") %>%
html_attr("href") %>%
na.omit() %>%
unique()
# Prepend base domain to relative URLs
full_links <- str_c("https://nc.milesplit.com", event_links)
return(full_links)
}, error = function(e) {
message("Error extracting links: ", base_url)
return(NULL)
})
}
# Main function to process the base page and all event links
process_main_page <- function(base_url) {
# Extract all event links from the main page
event_links <- extract_event_links(base_url)
# Initialize an empty list to store tables
all_tables <- list()
# Visit each event link and extract tables
for (link in event_links) {
tables <- extract_event_tables(link)
if (!is.null(tables)) {
all_tables <- append(all_tables, tables)
}
}
return(all_tables)
}
# Base URL for the results page
base_url <- "https://nc.milesplit.com/results"
# Run the scraping process
all_extracted_tables <- process_main_page(base_url)
url <- "https://nc.milesplit.com/meets/649296-cfacchs-polar-bear-2-2024/results"
# Read HTML content
page <- read_html(url)
# Find all tables with class 'eventTable'
tables <- page %>% html_nodes("table.eventTable")
url <- "https://nc.milesplit.com/meets/644574-reindeer-games-at-pchs-2025/results"
# Read HTML content
page <- read_html(url)
# Find all tables with class 'eventTable'
tables <- page %>% html_nodes("table.eventTable")
# Find all tables with class 'eventTable'
tables <- page %>% html_nodes("eventTable")
# Find all tables with class 'eventTable'
tables <- page %>% html_class("eventTable")
View(page)
# Find all tables with class 'eventTable'
tables <- page %>%html_node("table.eventTable") %>%  # Select table with class 'eventTable'
html_table(fill = TRUE)
# Read HTML content
page <- read_html(url)
# Find all tables with class 'eventTable'
tables <- page %>%html_node("table.eventTable") %>%  # Select table with class 'eventTable'
html_table(fill = TRUE)
page
# Find all tables with class 'eventTable'
tables <- page %>%html_node("table") %>%  # Select table with class 'eventTable'
html_table(fill = TRUE)
# Specify the URL
url <- "https://nc.milesplit.com/meets/644574-reindeer-games-at-pchs-2025/results"
# Read the webpage
page <- read_html(url)
# Extract tables
tables <- page %>%
html_nodes("table") %>%  # Find all HTML 'table' nodes
html_table(fill = TRUE)  # Convert to data frames
pages[[2]]
page[[2]]
page[[1]]
View(page)
page$node
# Load libraries
library(RSelenium)
install.packages("RSelenium")
# Load libraries
library(RSelenium)
# Start Selenium Server (Headless Chrome)
rD <- rsDriver(browser = "chrome", chromever = "latest", port = 4445L, verbose = FALSE)
install.packages("plumber")
install.packages("Rtools")
# Load required library
library(jsonlite)
# Convert finalMenBest.Rdata
load("finalMenBest.Rdata")
setwd("C:/Users/colem/Desktop/MilesplitProject/ECU/Runner-Recruiting-Resource/backend/Data")
# Convert finalMenBest.Rdata
load("finalMenBest.Rdata")
write(json_content, "finalMenBest.json")
json_content <- toJSON(finalMenBest, pretty = TRUE)
write(json_content, "finalMenBest.json")
# Convert finalMenBestE.Rdata
load("finalMenBestE.Rdata")
json_content <- toJSON(finalMenBestE, pretty = TRUE)
write(json_content, "finalMenBestE.json")
# Convert finalsMenBest.Rdata
load("finalsMenBest.Rdata")
json_content <- toJSON(finalsMenBest, pretty = TRUE)
write(json_content, "finalsMenBest.json")
# Convert finalsMenBestE.Rdata
load("finalsMenBestE.Rdata")
json_content <- toJSON(finalsMenBestE, pretty = TRUE)
write(json_content, "finalsMenBestE.json")
# Convert MenCalcTable.Rdata
load("MenCalcTable.Rdata")
json_content <- toJSON(MenCalcTable, pretty = TRUE)
# Convert MenCalcTable.Rdata
load("MenCalcTable.Rdata")
json_content <- toJSON(MenCalcTable, pretty = TRUE)
# Convert MenCalcTable.Rdata
load("MenCalcTable.Rdata")
# Convert MenViewTable.Rdata
load("MenViewTable.Rdata")
json_content <- toJSON(MenViewTable, pretty = TRUE)
# Convert Mile.Rdata
load("Mile.Rdata")
json_content <- toJSON(Mile, pretty = TRUE)
# Convert summariseMen.Rdata
load("summariseMen.Rdata")
json_content <- toJSON(summariseMen, pretty = TRUE)
write(json_content, "summariseMen.json")
# Convert summariseWomen.Rdata
load("summariseWomen.Rdata")
json_content <- toJSON(summariseWomen, pretty = TRUE)
write(json_content, "summariseWomen.json")
# Convert WomenCalcTable.Rdata
load("WomenCalcTable.Rdata")
json_content <- toJSON(WomenCalcTable, pretty = TRUE)
write(json_content, "WomenCalcTable.json")
# Convert WomenViewTable.Rdata
load("WomenViewTable.Rdata")
json_content <- toJSON(WomenViewTable, pretty = TRUE)
write(json_content, "WomenViewTable.json")
